{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DSBA/ITCS/ITIS 6162 - Programming Assignment\n",
    "\n",
    "### Name:\n",
    "\n",
    "### Submission instructions\n",
    "\n",
    "- Enter your name in the space above.\n",
    "- Save your completed notebook as *dsba6162_**\\<uncc username>**.ipynb*.\n",
    "- Upload **both** the **ipynb** file and the **html** version of your completed notebook to Canvas.\n",
    "\n",
    "You can download the notebook in html format by:\n",
    "- In browser, going to *File --> Download as --> HTML*.\n",
    "- In VSCode, expanding the '...' in the ribbon menu at the top of the file editor. *Export --> HTML*.\n",
    "\n",
    "***\n",
    "## Dataset\n",
    "\n",
    "The dataset for this assignment consists of all tweets between 01/01/2022 and 06/30/2022 containing any of the following keywords (case insensitive):\n",
    "- uncc\n",
    "- unccharlotte\n",
    "- unc charlotte\n",
    "- ninernation\n",
    "- niner nation\n",
    "\n",
    "Stored in *uncc_tweets.csv*.\n",
    "\n",
    "And the associated user interaction network derived from retweets, mentions, and references between users in the tweets.\n",
    "\n",
    "Stored as an edgelist in *uncc_graph.edgelist*.\n",
    "\n",
    "***\n",
    "\n",
    "# Section I. Topic Modeling on Texts using Latent Semantic Analysis (90 pts)\n",
    "*TF-IDF, SVD, Normalization, and K-means Clustering*\n",
    "\n",
    "In this section, you will try to discover a set of topics discussed in a dataset of tweets.\n",
    "\n",
    "For this task, you will use the *pandas* and *scikit-learn* packages to generate the TF-IDF matrix representing your dataset of texts. You will then apply TruncatedSVD to discover latent concepts in the data and reduce the dimensionality of the data matrix. Finally, you will use K-means clustering to group terms into clusters (topics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1 - Load Data and Exploratory Data Analysis (EDA) (20 pts)\n",
    "\n",
    "\n",
    "#### Part A - Read in tweets dataset\n",
    "\n",
    "Use the pandas library to read in the provided *uncc_tweets.csv* file.\n",
    "\n",
    "*Refer to the documentation of read_csv() if you're unsure*.\n",
    "\n",
    "- Make sure that the date column has the appropriate datatype (converted from string to datetime).\n",
    "- Make sure to check for and handle any missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part B - Time series plot\n",
    "\n",
    "For this part, refer to the pandas *GroupBy* documentation if you are unsure.\n",
    "\n",
    "1. Group your tweets dataframe by the date.\n",
    "2. Get the size of each date \"group\" (i.e. the number of tweets posted each day).\n",
    "3. Plot the time series of the number of tweets per day. Make sure your plot has appropriate axis labels and title.\n",
    "\n",
    "*Look in the sklearn documentation for a groupby function that returns the size of each group.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: What day had the maximal number of tweets mentioning UNCC?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2 - TF-IDF Matrix (10 pts)\n",
    "\n",
    "Create the initial TF-IDF matrix representation of our text dataset.\n",
    "\n",
    "**Required minimum preprocessing:**\n",
    "- Lowercase all text\n",
    "- Remove stop words\n",
    "- Set a maximum number of terms to include\n",
    "\n",
    "**Additional preprocessing options:**\n",
    "- Remove punctuation\n",
    "- Lemmatize the text\n",
    "- Use a more expansive stop word list\n",
    "- Tune the minimum and maximum document frequencies\n",
    "\n",
    "Python NLP libraries like *spaCy* or *NLTK* can provide additional text preprocessing and lemmatization functions.\n",
    "\n",
    "*Refer to the scikit-learn documentation regarding initialization parameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question: How many documents and how many terms are represented in our TF-IDF matrix?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3 - Latent Factor Discovery and Dimensionality Reduction (30 pts)\n",
    "\n",
    "Such a high dimensional matrix is not well-suited for clustering, and individual terms do not make for very informative features. Use sklearn's TruncatedSVD to discover informative latent factors in our data and reduce the dimensionality of the document matrix.\n",
    "\n",
    "#### Part A - Determine an appropriate number of components\n",
    "\n",
    "1. Compute the singular value decomposition of the TF-IDF matrix using a (relatively) high number of components.\n",
    "\n",
    "*Note: Higher numbers of components can take a much longer time to run. For reference, 500 components took ~30s on my computer.*\n",
    "\n",
    "*To save on some compute, you can use just .fit() for this instead of fit_transform() since we will not be using the transformed matrix from this step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Plot **at least one** of the following quantities:\n",
    "    - Singular values\n",
    "    - Explained variance\n",
    "    - Explained variance ratio\n",
    "\n",
    "*Refer to the scikit-learn documentation for TruncatedSVD to see where to find these values.*\n",
    "\n",
    "**Question: What do you notice about the shape of the curve? In what other context have you seen a similar plot in this course?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional*: Plot the cumulative sum of the explained variance ratio. This will show you the total percentage of variance of the original matrix (y-axis) that you will capture in the reduced form matrix of size *n* components (x-axis).\n",
    "\n",
    "*I find this to be a more interpretable version of the same information presented in the previous plot.*\n",
    "\n",
    "*Hint: Look at the np.cumsum() function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Choose what you think is an appropriate number of components based on the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part B - Compute the reduced form of the TF-IDF matrix\n",
    "\n",
    "Recompute the SVD of the TF-IDF matrix with your selected number of components and transform the result to the reduced dimension matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question: What is the total percentage of explained variance of the reduced-dimension matrix?**\n",
    "\n",
    "*To get the total explained variance percentage of the approximated matrix, compute the sum of explained variance over each individual component in the matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part C - Unnormalized vs. Normalized, Features vs. Samples\n",
    "\n",
    "There are many different ways to re-scale or normalize data. Most often, empirical testing is the best way to determine what normalization technique is appropriate for a given application.\n",
    "\n",
    "- The **Normalizer()** class or **normalize** function provides normalization across *samples* (rows).\n",
    "- The **StandardScaler()** class provides standardizing across *features* (columns).\n",
    "\n",
    "Choose *at least one* of the techniques above. Compute a re-scaled version of the data matrix for comparison against the unnormalized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 4 - Clustering and Topic Modeling (30 pts)\n",
    "\n",
    "Cluster the documents using the K-means algorithm, and examine the top terms associated with each cluster.\n",
    "\n",
    "#### Part A: Determine an appropriate k-value\n",
    "\n",
    "1. Run k-means for a series of reasonable k-values. Store the sum of squared distances of samples to their closest cluster center (available from the fitted sklearn estimator) for each k-value.\n",
    "2. Plot the sum of squared distances vs k-value.\n",
    "3. Determine what you think to be an appropriate value for k.\n",
    "\n",
    "*To save a little compute, you can set n_init = 1 for the Kmeans estimator for this step.*\n",
    "\n",
    "*Note: In my testing, the \"elbow\" curve in this case does not have a very good bend. Just use your best judgement to determine an appropriate k-value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part B: Cluster the document matrix\n",
    "\n",
    "Fit a K-means estimator with your selected number of clusters to the **unnormalized**, reduced-dimension document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Fit a K-means estimator with your selected number of clusters to the **normalized**, reduced-dimension document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part C: Cluster analysis\n",
    "\n",
    "The function below will print out the top *n* words associated with each cluster centroid. The function requires the fitted sklearn estimators for TF-IDF, SVD, and K-means.\n",
    "\n",
    "Print out the top terms associated with each cluster centroid generated from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_top_terms_per_cluster(vectorizer: TfidfVectorizer, svd: TruncatedSVD, kmeans: KMeans, n_terms: int = 20):\n",
    "    original_space_centroids = svd.inverse_transform(kmeans.cluster_centers_)\n",
    "    cluster_word_indices = original_space_centroids.argsort()[:, :-(n_terms+1):-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print('Top terms per cluster:')\n",
    "    for cluster_num, word_indices in enumerate(cluster_word_indices):\n",
    "        print(f'Cluster {cluster_num}:')\n",
    "        for i in word_indices:\n",
    "            print(f'\\t{terms[i]}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Choose three clusters from your results above on the dataset. Based on the top *n* terms, what general topic describes each cluster?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How do the clusters differ between the unnormalized and normalized data matrices?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "\n",
    "# Section II. Community Detection on Social Networks (60 pts)\n",
    "\n",
    "*Degree Centrality, PageRank, Community Detection, and Partition Quality*\n",
    "\n",
    "In this section, you will compute PageRank on a social network to determine the most \"influential\" nodes in the network. You will also perform community detection to identify clusters (communities) of nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1: Load data and construct graph (10 pts)\n",
    "\n",
    "The data are stored in an edgelist file. Use the networkx *read_edgelist()* function to read the file into a networkx Graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print the total number of nodes and edges in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Degree Centrality and PageRank (20 pts)\n",
    "\n",
    "*Note: Due to Twitter's TOS, I had to anonymize users in the graph, so we can't go beyond determining the most influential nodes. Generally, we would conduct further analysis to investigate the top nodes returned by these centrality metrics.*\n",
    "\n",
    "There are many centrality metrics out there for measuring the most \"important\" nodes in a network. For this analysis, we will use degree centrality and PageRank (akin to eigenvector centrality).\n",
    "\n",
    "#### Part A: Degree centrality\n",
    "\n",
    "1. Compute the degree centrality for all nodes in the graph.\n",
    "2. Print out the top 20 nodes with the highest degree centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part B: PageRank\n",
    "\n",
    "1. Compute PageRank for all nodes in the graph.\n",
    "2. Print out the top 20 with the highest PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question: Is the set of top nodes the same for both centrality metrics?**\n",
    "\n",
    "*Optional: Test a few other centrality metrics or the HITS algorithm. What nodes are the most important based on these metrics?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3: Community Detection (30 pts)\n",
    "\n",
    "Cluster the nodes in the network into communities using a modularity-based approach and a label propagation-based approach. As community detection is an unsupervised problem, one of the major challenges is evaluating the quality of a graph partition generated by a clustering algorithm.\n",
    "\n",
    "Networkx provides two metrics for measuring the quality of a partition into communities.\n",
    "- Modularity.\n",
    "- Partition Quality (a composite of *coverage* and *performance* of a partition).\n",
    "\n",
    "#### Part A: Greedy modularity maximization\n",
    "\n",
    "1. Run the greedy modularity community detection algorithm on the network.\n",
    "2. Compute the **modularity** and **partition quality** for the clustering result.\n",
    "\n",
    "*Note: This can take several minutes to run. For reference, it took about ~1.5 minutes on my computer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Greedy-Mod clustering will return clusters in decreasing size.\n",
    "\n",
    "3. Plot the size of clusters. (The curve should be a familiar shape).\n",
    "\n",
    "**Question: How many total clusters did the modularity-based approach produce? What are the sizes of the top 10 clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part B: Label propagation method\n",
    "\n",
    "(I would have liked to use the Girvan-Newman method since that is discussed more in the slides, but its scaling is very poor and 10,000 nodes is the upper limit of what's reasonable for that algorithm.)\n",
    "\n",
    "Another semi-supervised method (used for many things besides community detection) is label propagation. In the label propagation algorithm, each node is initially assigned to a unique community label. At each step in the algorithm, the labels of every node are propagated to their neighbors, and each node's label is updated as the most frequent label of all its neighbors. The algorithm terminates when the labels converge, i.e. no nodes change labels from one iteration to the next.\n",
    "\n",
    "1. Run the label propagation community detection algorithm on the network.\n",
    "2. Compute the **modularity** and **partition quality** for the clustering result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Note: Label propagation does not return communities in sorted order like greedy mod. You will need to sort the communities by size first.*\n",
    "\n",
    "3. Plot the size of clusters.\n",
    "\n",
    "**Question: How many clusters did the label propagation-based approach produce? What are the sizes of the top 10 clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question: Which method produced a node partition with the higher modularity? Which produced the partition with the higher partition quality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Considering both the differences in clustering quality and runtime complexity, which community detection method would you prefer for analysis of large graphs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dsba-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "79d97e69620ddd17cca58958f99d2a759090af75357fa95af3d6d0c788466ee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
